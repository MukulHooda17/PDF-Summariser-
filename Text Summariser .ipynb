{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import networkx as nx  # Add this import\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "import sklearn\n",
    "\n",
    "# Getting nltk\n",
    "import nltk\n",
    "\n",
    "# Importing word_tokenize to tokenize words in a sentence\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Getting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "stop_words = []\n",
    "stop_words = set(stop_words)\n",
    "# stopwords.words('english')\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "# sklearn_stopwords\n",
    "\n",
    "total_stopwords = nltk_stopwords.union(sklearn_stopwords)\n",
    "final_stopwords = total_stopwords.union(stop_words)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=final_stopwords):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    \n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    "    \n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    for word in sent1:\n",
    "        if word not in stopwords:\n",
    "            vector1[all_words.index(word)] += 1\n",
    "    \n",
    "    for word in sent2:\n",
    "        if word not in stopwords:\n",
    "            vector2[all_words.index(word)] += 1\n",
    "    \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def generate_summary(text, max_words=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    sentence_similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sentence_similarity_matrix[i][j] = sentence_similarity(\n",
    "                    sentences[i].split(), sentences[j].split(), stop_words)\n",
    "    \n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    \n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    summary = ''\n",
    "    word_count = 0\n",
    "    for i in range(len(ranked_sentences)):\n",
    "        summary += ranked_sentences[i][1] + ' '\n",
    "        word_count += len(ranked_sentences[i][1].split())\n",
    "        if word_count >= max_words:\n",
    "            break\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your PDF document\n",
    "pdf_file_path = '/Users/mukulhooda/Downloads/Operations Management.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the PDF document\n",
    "document_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "# Generate a summary under 500 words\n",
    "document_summary_1 = generate_summary(document_text, max_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3abf5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the summarized text\n",
    "print(document_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378a82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552626f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff15239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe06b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random  # Import the random module\n",
    "\n",
    "# Set a seed for the random number generator\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = ''\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)  # Use PdfReader instead of PdfFileReader\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to perform extractive summarization using TextRank\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    cosine_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Create a graph representation of sentences\n",
    "    graph = nx.from_numpy_array(cosine_matrix)\n",
    "\n",
    "    # Calculate sentence importance scores using PageRank (TextRank)\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    # Sort sentences by importance score and select the top ones\n",
    "    ranked_sentences = sorted(((scores[i], sentence) for i, sentence in enumerate(sentences)), reverse=True)\n",
    "    summary = ' '.join([sentence for _, sentence in ranked_sentences[:num_sentences]])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    pdf_file = '/Users/mukulhooda/Downloads/Operations Management.pdf'\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    summary = summarize_text(text, num_sentences=100)\n",
    "    print(\"Summary:\")\n",
    "    print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8211f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from operator import itemgetter\n",
    "import sklearn\n",
    "import re  # Add this import for regular expressions\n",
    "\n",
    "# Getting nltk\n",
    "import nltk\n",
    "\n",
    "# Importing word_tokenize to tokenize words in a sentence\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Getting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "stop_words = []\n",
    "stop_words = set(stop_words)\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "total_stopwords = nltk_stopwords.union(sklearn_stopwords)\n",
    "final_stopwords = total_stopwords.union(stop_words)\n",
    "\n",
    "# Define a regular expression pattern to identify headings and subheadings\n",
    "heading_pattern = r'^(#+)\\s+(.*?)$'\n",
    "\n",
    "def extract_headings_and_text(text):\n",
    "    headings_and_text = []\n",
    "    lines = text.split('\\n')\n",
    "    current_heading = None\n",
    "    current_text = []\n",
    "\n",
    "    for line in lines:\n",
    "        match = re.match(heading_pattern, line)\n",
    "        if match:\n",
    "            # If a heading is found, save the previous heading and text (if any)\n",
    "            if current_heading and current_text:\n",
    "                headings_and_text.append((current_heading, '\\n'.join(current_text)))\n",
    "            # Set the current heading and clear the text\n",
    "            current_heading = match.group(1)\n",
    "            current_text = []\n",
    "        else:\n",
    "            # If not a heading, append the line to the current text\n",
    "            current_text.append(line)\n",
    "\n",
    "    # Add the last heading and text (if any)\n",
    "    if current_heading and current_text:\n",
    "        headings_and_text.append((current_heading, '\\n'.join(current_text)))\n",
    "\n",
    "    return headings_and_text\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=final_stopwords):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    for word in sent1:\n",
    "        if word not in stopwords:\n",
    "            vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sent2:\n",
    "        if word not in stopwords:\n",
    "            vector2[all_words.index(word)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def generate_summary(text, max_words=500):\n",
    "    headings_and_text = extract_headings_and_text(text)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    sentence_similarity_matrix = np.zeros((len(headings_and_text), len(headings_and_text)))\n",
    "\n",
    "    for i in range(len(headings_and_text)):\n",
    "        for j in range(len(headings_and_text)):\n",
    "            if i != j:\n",
    "                sentence_similarity_matrix[i][j] = sentence_similarity(\n",
    "                    word_tokenize(headings_and_text[i][1]),\n",
    "                    word_tokenize(headings_and_text[j][1]),\n",
    "                    stop_words)\n",
    "\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    ranked_headings_and_text = sorted(((scores[i], h, t) for i, (h, t) in enumerate(headings_and_text)),\n",
    "                                      key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    summary = ''\n",
    "    word_count = 0\n",
    "    for i in range(len(ranked_headings_and_text)):\n",
    "        summary += ranked_headings_and_text[i][1] + '\\n' + ranked_headings_and_text[i][2] + '\\n\\n'\n",
    "        word_count += len(word_tokenize(ranked_headings_and_text[i][2]))\n",
    "        if word_count >= max_words:\n",
    "            break\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Getting nltk\n",
    "import nltk\n",
    "\n",
    "# Importing word_tokenize to tokenize words in a sentence\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Getting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "# Combine NLTK and sklearn stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "total_stopwords = nltk_stopwords.union(sklearn_stopwords)\n",
    "final_stopwords = total_stopwords.union(stop_words)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=final_stopwords):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    for word in sent1:\n",
    "        if word not in stopwords:\n",
    "            vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sent2:\n",
    "        if word not in stopwords:\n",
    "            vector2[all_words.index(word)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def generate_summary(text, max_words=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Initialize a flag to check if a sentence is a heading or subheading\n",
    "    is_heading = False\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence.isupper():  # Check if the sentence is in all uppercase (a heading)\n",
    "            is_heading = True\n",
    "            summary += sentence + '\\n'  # Add a line break after the heading\n",
    "        elif sentence.strip() == '':  # Check if the sentence is empty (end of a paragraph)\n",
    "            is_heading = False\n",
    "        elif is_heading:\n",
    "            summary += sentence + '\\n'  # Add a line break after a subheading\n",
    "        else:\n",
    "            summary += sentence + ' '\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = pdf_file_path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "summary = generate_summary(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "# Getting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "# Combine NLTK and sklearn stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "total_stopwords = nltk_stopwords.union(sklearn_stopwords)\n",
    "final_stopwords = total_stopwords.union(stop_words)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=final_stopwords):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    for word in sent1:\n",
    "        if word not in stopwords:\n",
    "            vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sent2:\n",
    "        if word not in stopwords:\n",
    "            vector2[all_words.index(word)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def generate_summary(text, max_words=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Initialize a flag to check if a sentence is a heading or subheading\n",
    "    is_heading = False\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence contains mostly uppercase words (likely a heading or subheading)\n",
    "        if re.match(r'^[A-Z\\s]+$', sentence.strip()):\n",
    "            is_heading = True\n",
    "            summary += sentence + '\\n'  # Add a line break after the heading\n",
    "        elif sentence.strip() == '':  # Check if the sentence is empty (end of a paragraph)\n",
    "            is_heading = False\n",
    "        elif is_heading:\n",
    "            summary += sentence + '\\n'  # Add a line break after a subheading\n",
    "        else:\n",
    "            summary += sentence + ' '\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = pdf_file_path\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "summary = generate_summary(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316f4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=summary\n",
    "# Split the text into paragraphs based on double line breaks (empty lines)\n",
    "paragraphs = text.split('\\n\\n')\n",
    "\n",
    "# Create a PDF document\n",
    "pdf_document = fitz.open()\n",
    "\n",
    "# Loop through paragraphs and add them as text to the PDF with line breaks\n",
    "for paragraph in paragraphs:\n",
    "    pdf_page = pdf_document.new_page()\n",
    "    pdf_page.insert_text(fitz.Point(50, 50), paragraph, fontname=\"helvetica\")\n",
    "\n",
    "# Save the PDF to a file\n",
    "output_pdf_path = 'summary_output.pdf'\n",
    "pdf_document.save(output_pdf_path)\n",
    "pdf_document.close()\n",
    "\n",
    "print(\"PDF file saved:\", output_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70add2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c57c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c94f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbfeeadb",
   "metadata": {},
   "source": [
    "# PDF Summarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686020e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your pdf file: /Users/mukulhooda/Desktop/Projects/My Projects /Project --> PDF Summarizer /Operations Management.pdf\n",
      "Operations Management: Oil and Gas\n",
      "Report\n",
      "Introduction\n",
      "Operations management is a branch of management that deals with the\n",
      "designing and supervision of operational processes in a business organization. Operations management covers the responsibility over all processes that involve\n",
      "the production of goods and services as well as the delivery of such productions\n",
      "to the final consumers. In its duties, an operations management department ensures that processes are\n",
      "planned for and executed in an efficient and effective way to satisfy the needs of\n",
      "the organization and its customers. This paper seeks to discuss concepts of\n",
      "operations management. The paper will look into the history, functions, case studies, advantages,\n",
      "disadvantages and factors that affect the department among others. The paper\n",
      "will then look into the operations management’s involvement in oil and gas\n",
      "companies. Operations Management\n",
      "Business enterprises entail the provision of goods and services to their\n",
      "immediate customers. For the finished goods or offered services to be available\n",
      "to consumers in a state that will satisfy the needs and desires of the consumers,\n",
      "measures must be undertaken by the producing organization to ensure that\n",
      "quality, quantity as well as the time frame of the production is appropriate with\n",
      "respect to the demands of consumers. Meeting the needs of consumers is, however, a process that begins with the\n",
      "search for raw materials which are then processed to be goods and finally\n",
      "supplied to the consumers. Processes of activities such as extraction of raw materials or resources, their\n",
      "transportation, their processing and their final distribution involve operational\n",
      "activities. It is the move to supervise and manage these activities that derives the\n",
      "basis of operations management. Operations management ensures “effective\n",
      "management of resources and activities that produce or deliver goods and\n",
      "services of any business” (Sox 1). Operations management therefore involves the management of “people,\n",
      "materials, equipments and information resources that a business may need” (Sox\n",
      "1) in its daily activities. The department thus outlines and then manages all that\n",
      "pertains to the production of goods and services. The operations management is actually dominant in almost every stage of any\n",
      "given supply chain and is diverse with a variety of titles that at time can include\n",
      "“production planner, inventory manager, logistics manager, procurement\n",
      "manager and supply chain manager” (Sox 1) among others. History of Operations Management\n",
      "The history of operations management stems all the way back to the eighteenth\n",
      "century. In the management of production activities, operations changes were, for\n",
      "example, realized in the labor system. In England, for instance, the textile\n",
      "industry registered operational changes with human labor being replaced with the\n",
      "use of machines. Inventions of industrial equipments also lead to adjustment in\n",
      "methods of production in the textile industry at the time. In the year 1785, steam engine was invented providing more options in the\n",
      "operations field. Administration of operations activities in business aspects,\n",
      "however, took its significant development in the twentieth century with\n",
      "introduction of theories and principles over how operations should be sufficiently\n",
      "managed. \n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "# Getting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "# Combine NLTK and sklearn stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "total_stopwords = nltk_stopwords.union(sklearn_stopwords)\n",
    "final_stopwords = total_stopwords.union(stop_words)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_document[page_num]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=final_stopwords):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [word.lower() for word in sent1]\n",
    "    sent2 = [word.lower() for word in sent2]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    for word in sent1:\n",
    "        if word not in stopwords:\n",
    "            vector1[all_words.index(word)] += 1\n",
    "\n",
    "    for word in sent2:\n",
    "        if word not in stopwords:\n",
    "            vector2[all_words.index(word)] += 1\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def generate_summary(text, max_words=500):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Initialize a flag to check if a sentence is a heading or subheading\n",
    "    is_heading = False\n",
    "    summary = ''\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if the sentence contains mostly uppercase words (likely a heading or subheading)\n",
    "        if re.match(r'^[A-Z\\s]+$', sentence.strip()):\n",
    "            is_heading = True\n",
    "            summary += sentence + '\\n'  # Add a line break after the heading\n",
    "        elif sentence.strip() == '':  # Check if the sentence is empty (end of a paragraph)\n",
    "            is_heading = False\n",
    "        elif is_heading:\n",
    "            summary += sentence + '\\n'  # Add a line break after a subheading\n",
    "        else:\n",
    "            # Calculate the word count in the current sentence\n",
    "            word_count += len(sentence.split())\n",
    "            # If adding the sentence doesn't exceed the word limit, add it to the summary\n",
    "            if word_count <= max_words:\n",
    "                summary += sentence + ' '\n",
    "            else:\n",
    "                break  # Stop summarizing when the word limit is reached\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "pdf_path = input('Enter your pdf file: ')\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "summary = generate_summary(text, max_words=500)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3269c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file saved: summary_output.pdf\n"
     ]
    }
   ],
   "source": [
    "text=summary\n",
    "# Split the text into paragraphs based on double line breaks (empty lines)\n",
    "paragraphs = text.split('\\n\\n')\n",
    "\n",
    "# Create a PDF document\n",
    "pdf_document = fitz.open()\n",
    "\n",
    "# Loop through paragraphs and add them as text to the PDF with line breaks\n",
    "for paragraph in paragraphs:\n",
    "    pdf_page = pdf_document.new_page()\n",
    "    pdf_page.insert_text(fitz.Point(50, 50), paragraph, fontname=\"helvetica\")\n",
    "\n",
    "# Save the PDF to a file\n",
    "output_pdf_path = 'summary_output.pdf'\n",
    "pdf_document.save(output_pdf_path)\n",
    "pdf_document.close()\n",
    "\n",
    "print(\"PDF file saved:\", output_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e2e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562b9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
